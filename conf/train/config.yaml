defaults:
  - hydra/launcher: slurm
  - dataset: hammers
  - slurm: v2

use_ddp: True
world_size: 1
rank: 0
local_rank: 0

n_epochs: 300000
batch_size: 1 # samples sent through 1-by-1, step optimizer every batch_size samples, per-gpu
resume_ckpt: ''        # path to model checkpoint to resume
resume_ckpt_opt_state: ''
workers: 1
num_kp: 8
outf: '/root/checkpoint'
lr: 0.0001
num_cates: 1
category: 1       # category to train
num_points: 500   # number of point-cloud points in input

log_every_n_samples: 99
checkpoint_every_n_samples: 3960

loss_sep_type: coverage # euclidean, geodesic, coverage or curvature
loss_surf_type: volume # surface or volume
loss_term_weights:
  loss_att_weight: 0.
  Kp_dis_weight: 0.0001
  Kp_cent_dis_weight: 0.
  loss_rot_weight: 0.0
  loss_surf_weight: 60.0
  loss_sep_weight: 3.0
  kp_to_mesh_dist_scale: 10.0

scale_loss_inputs_by: 0.1

slurm_additional:
  wandb_dir: '/root/checkpoint'
  python_bin: '/opt/conda/bin/conda run -n 6pack python'
  python_optstr: '-m torch.distributed.launch --master_port 99`expr $SLURM_JOB_ID % 20 + 20` --nproc_per_node=${slurm_additional.gpus}'
  gpus: 4
  checkpoint_dir: /h/dturpin/checkpoint/$SLURM_JOB_ID

singularity:
  bin_path: '/h/dturpin/pkgs/bin/singularity'
  sbox_path: '/h/dturpin/img/6pack_sbox'
  binds:
    - /h/dturpin/checkpoint/$SLURM_JOB_ID:/root/checkpoint
    - /h/dturpin/tmp:/tmp

exec_path: '/h/dturpin/repos/6-PACK-latest/scripts/train.py'
